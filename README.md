# NLP_Review
## Prerequisite 1. CS224n Winter 2019(KR)
|Title|Summary|Related Papers|
|------|---|---|
|Introductions & Word Vectors|[Lecture 1](https://github.com/hahajjjun/NLP_Review/blob/main/Lecture%20Notes/Lecture%201/Lecture%201.md)|[Word2Vec](https://arxiv.org/abs/1301.3781)|
|Word Vectors & Word Senses|[Lecture 2](https://github.com/hahajjjun/NLP_Review/blob/main/Lecture%20Notes/Lecture%202/Lecture%202.md)|[GloVe](https://aclanthology.org/D14-1162.pdf)|
|Neural Networks|[Lecture 3](https://github.com/hahajjjun/NLP_Review/blob/main/Lecture%20Notes/Lecture%203/Lecture%203.md)||
|Backpropagation|[Lecture 4]||
|Dependency Parsing|[Lecture 5](https://github.com/hahajjjun/NLP_Review/blob/main/Lecture%20Notes/Lecture%205/Lecture%205.md)||
|Language Models & RNNs|Lecture 6||
|Vanishing Gradients & Fancy RNNs|Lecture 7||
|Translation & Seq2Seq & Attention|Lecture 8||
|Practical Tips|Lecture 9||
|Q&A|Lecture 10||
|Convolutional Networks for NLP|Lecture 11||
|Subword Models|Lecture 12||
|Contextual Word Embeddings|Lecture 13||
|Transformers & Self-Attention|Lecture 14||
|Natural Language Generation|Lecture 15||
|Coreference Resolution|Lecture 16||
|Multitask Learning|Lecture 17||
|Constituency Parsing & TreeRNN|Lecture 18||
|Bias in AI|Lecture 19||
|Future of NLP & DL|Lecture 20||
|Low-Resource Machine Translation|Lecture 21||
|BERT & Other Pre-trained Language Models|Lecture 22||

## Prerequisite 2. Fundamental NLP Papers
- Universal Language Model Fine-tuning for Text Classification
- Deep contextualized word representations
- Attention is All You Need
- BERT: Pre-training of Deep bidirectional Transformers for Language Understanding
