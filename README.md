# NLP_Review
## Prerequisite 1. CS224n : Summary in Korean
|Title|Summary|Related Papers|
|------|---|---|
|Introductions and Word Vectors|[Lecture 1](https://github.com/hahajjjun/NLP_Review/blob/main/Lecture%201/Lecture%201.md)|[Word2Vec](https://arxiv.org/abs/1301.3781)|

## Prerequisite 2. Fundamental NLP Papers
- Universal Language Model Fine-tuning for Text Classification
- Deep contextualized word representations
- Attention is All You Need
- BERT: Pre-training of Deep bidirectional Transformers for Language Understanding
