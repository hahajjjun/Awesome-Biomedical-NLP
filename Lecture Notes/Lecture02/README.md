# Lecture 2

### Word Vectors and Word Senses

---

### Overview : ì—¬ëŸ¬ ê°€ì§€ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸

|Index|Subtitle|
|--- | --- |
|2.1.| Word2Vec recap |
|2.2.| Word2Vec algorithm family |
|2.3.| LinAlg based models : counting |
|2.4.| GloVe model |
|2.5.| Intrinsic/Extrinsic Evaluation of word vectors |
|2.6.| Word senses |

---

### 2.1. Word2Vec recap

- Word VectorëŠ” **ëœë¤í•˜ê²Œ ì´ˆê¸°í™”**ëœ ìƒíƒœ
- ì „ì²´ **corpusë¥¼ ìˆœíšŒ**í•¨, sliding window
- Modelì€ **center word vector â‡’ outer context word vectorë¥¼ ì¶”ì •**í•˜ëŠ” ì¼ì„ ìˆ˜í–‰
- ê·¸ ê³¼ì •ì—ì„œ optimizationë˜ëŠ” parameter = word vector embeddingì— í•´ë‹¹
- í•™ìŠµ ê³¼ì • = word vectorë¥¼ updateí•´ì„œ surrounding wordë¥¼ ë” ì˜ ì¶”ì •í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê³¼ì •
- P(o|c) = softmax(dot(uo, vc))ì´ë¯€ë¡œ **matrix notation**ìœ¼ë¡œë„ í‘œí˜„ ê°€ëŠ¥

<aside>
ğŸ’¡ Why?

Word Vector Embedding ê²°ê³¼ë¥¼ 2D, 3D Projectioní•´ì„œ clusteringí–ˆì„ ë•Œ ìœ ì‚¬í•œ ë‹¨ì–´ë¼ë¦¬ ë¹„ìŠ·í•œ ìœ„ì¹˜ì— ì¡´ì¬í•˜ëŠ” ì´ìœ 

â‡’  ë‹¨ì–´ê°€ ì˜ë¯¸ì ìœ¼ë¡œ í˜¸ì‘í•˜ì§€ ì•Šìœ¼ë©´ ê°™ì€ window ë‚´ ë“±ì¥ì´ ì ì–´ P(o|c)ë¥¼ ì‘ê²Œ ì¶”ì •í•´ì•¼ í•˜ë¯€ë¡œ

â‡’  ë‘ word vectorì˜ dot productê°€ ê°ì†Œí•¨(projectioní•´ë„ ì„œë¡œ ë‹¤ë¥¸ ë°©í–¥ì„ ê°€ë¦¬í‚¤ê²Œ ë¨)

</aside>

- ë‹¨ì–´ì˜ ìˆœì„œì™€ ìœ„ì¹˜ëŠ” ê³ ë ¤í•˜ì§€ ì•Šê³ , ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ì— ì˜ì¡´ â†’ **Bag of Words modelê³¼ë„ ìœ ì‚¬í•¨**
- Optimization : Gradient Descent (single updateí•  ë•Œë„ ì—°ì‚°ì´ ë§ì´ í•„ìš”í•¨)
    - Stochastic Gradient Descentì„ í†µí•´ì„œ sample window í˜¹ì€ small batchì— ëŒ€í•´ì„œ updateì‹œí–‰
    - Very Sparse Gradient Vector

---

### 2.2. Word2Vec algorithm family

- **Word Vector Embedding ê°œìˆ˜ì— ë”°ë¥¸ ë¶„ë¥˜**
    - ì§€ê¸ˆê¹Œì§€ ì„¤ëª…í•œ Word2Vec ëª¨ë¸ì€ **í•œ ë‹¨ì–´ë‹¹ ë‘ ê°œì˜ word vector**(center, outside(context))ê°€ ì„ë² ë”©í•¨ â‡’ ìµœì¢… word vector ì‚°ì¶œ ì‹œì—ëŠ” ë‘ word vectorì˜ í‰ê· ìœ¼ë¡œ êµ¬í•˜ê²Œ ë¨
        - ì§ê´€ì ì´ê³  ë‹¨ìˆœí•œ ë°©ë²•
    - **í•œ ë‹¨ì–´ë‹¹ í•œ ê°œì˜ word vector embedding**ì„ ìˆ˜í–‰í•˜ëŠ” variated model
        - slightly better performance
        - much complicated calculus of gradient
- **Loss functionì„ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì— ë”°ë¥¸ ë¶„ë¥˜**
    - Skip-Grams(SG)
        - center wordê°€ ì£¼ì–´ì§€ë©´ â‡’ outside wordë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìˆœì„œ : P(o|c)
    - Continuous Bag of Words(CBOS)
        - context wordsë¥¼ ë°”íƒ•ìœ¼ë¡œ â‡’ center wordë¥¼ ì—ì¸¡í•˜ëŠ” ìˆœì„œ : P(c|o)
- **ê·¸ ì™¸ Additional Variations**
    - Negative Sampling
        - Naive Softmax ê°œì„  â‡’ Loss functionì— logistic term ì¶”ê°€
        - **ì•„ì´ë””ì–´ : ì¢‹ì€ ëª¨ë¸ì´ë¼ë©´ true pair(c,o)ì™€ random noise(c,o)ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤!**
            
<p align = "center">
<img src = "../../Figures/Lecture02/Fig 1.png" width = "600dp"/>
</p>
            

---

### 2.3. LinAlg based models : counting

- ***COALS; Rohde, Gonnerman, & Plaut, 2005***
    - Co-occurence matrixë¥¼ êµ¬ì„± â‡’ SVD ê°™ì€ LinAlg based calculation í™œìš©
    - **Co-occurence matrix building ë°©ë²•**
        - **Sliding windows**
            - Word2Vecê³¼ ìœ ì‚¬, ê° ë‹¨ì–´ë§ˆë‹¤ windowë¥¼ ì„¤ì •í•´ slide
            - **syntactic, semantic informationì„ captureí•  ìˆ˜ ìˆìŒ**
                - syntactic ì˜ˆì‹œ : fly â‡’ flying : eat â‡’ ?
                - semantic ì˜ˆì‹œ : man â‡’ king : woman â‡’ ?
        - **Full document**
            - ì „ì²´ docì— ëŒ€í•œ co-occurence matrixëŠ” ì „ì²´ textì˜ generalí•œ topicì„ ì•Œë ¤ì¤Œ
            - **Latent Semantic Analysis(LSA)ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ**
        - Co-occurence matrix ì „ì²´ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ : But very high dimensional & Sparse
            - less robust model & sparsity issue
            - **SVD í†µí•œ ì°¨ì› ì¶•ì†Œ**

<p align = "center">
<img src = "../../Figures/Lecture02/Fig 2.png" width = "400dp"/>
</p>

- Co-occurence matrix Xë¥¼ êµ¬ì„±í•  ë•Œ ì‚¬ìš©í•œ Hacks
    - Raw countì— ëŒ€í•´ì„œ SVDë¥¼ ìˆ˜í–‰í•˜ë©´ ì‚¬ì‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ
    - Scaling the counts
        - ë¹ˆë„ê°€ ë„ˆë¬´ ë†’ì€ function wordsì œì™¸
        - ë¹ˆë„ = max(X,100)ìœ¼ë¡œ thresholdí•˜ê¸°
        - log(ë¹ˆë„)ë¡œ ê³„ì‚°
    - ë‹¨ìˆœíˆ count ì“°ê¸°ë³´ë‹¤ Pearson correlation ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ë¥´ê²Œ(negative vals â‡’ 0)
    - ì¤‘ì‹¬ì—ì„œ ê°€ê¹ê²Œ ë‚˜íƒ€ë‚ ìˆ˜ë¡ ë” ë†’ì€ ê°€ì¤‘ì¹˜ : Ramped Windows(ğŸ”º)

---

### 2.4. GloVe Model

- **ì§€ê¸ˆê°€ì§€ ì†Œê°œí•œ ëª¨ë¸ë“¤ ë¶„ë¥˜í•´ì„œ ì •ë¦¬í•œ ê²°ê³¼**
    
    
    | Count Based | Direct Prediction |
    | --- | --- |
    | LSA(Latent Semantic Analaysis), HAL | Skip-Gram & CBOW |
    | COALS, Hellinger-PCA | NNLM, HLBL, RNN |
    | ì¥ì  1 : Fast Training | ì¥ì  1 : Improved performance |
    | ì¥ì  2 : Efficient Usage of statistics | ì¥ì  2 : Capture complex patterns |
    | ë‹¨ì  1 : ë‹¨ìˆœí•œ Word similarity ì •ë„ë§Œ capture ê°€ëŠ¥ | ë‹¨ì  1 : Corpus Sizeì— ë”°ë¼ í•™ìŠµ ì†ë„ ëŠë ¤ì§ |
    | ë‹¨ì  2 : ë¹ˆë„ê°€ ë†’ê²Œ ì¸¡ì •ëœ ë‹¨ì–´ì— importance ë¶ˆê· ë“±í™” | ë‹¨ì  2 : Inefficient Usage of Statistics |
- ***Encoding meaning components in vector differences [Pennington et al, EMNLP 2014]***
    - Co-occurence probabilityì˜ ë¹„ìœ¨ì´ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸ì½”ë”©í•  ê²ƒì´ë¼ê³  ì¶”ë¡ í•¨
        
<p align = "center">
<img src = "../../Figures/Lecture02/Fig 3.png" width = "400dp"/>
</p>

- Counting í†µí•´ì„œ embedding vector ê²Œì‚°í•˜ê±°ë‚˜ ìœ ì¶” ê°€ëŠ¥
        
<p align = "center">
<img src = "../../Figures/Lecture02/Fig 4.png" width = "300dp"/>
</p>
        
- ***GloVe [Pennington et al, EMNLP 2014]***
    <p align = "center">
    <img src = "../../Figures/Lecture02/Fig 5.png" width = "400dp"/>
    </p>

    - Fast Training / Scalabilityê°€ ì¥ì 
    - ì‘ì€ ê·œëª¨ì˜ Corpusì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

---

### 2.5. Intrinsic/Extrinsic evaluation of word vectors
- Intrinsic
    - ì „ì²´ modelì˜ taskì¤‘ ì¼ë¶€ë¶„ì¸ subtaskì— ëŒ€í•´ì„œ performace measure
    - ë¹ ë¥´ê³ , ë¹„ìš©ì´ ì ê²Œ ë“¦
    - Real taskì™€ì˜ correlationì„ ì‚´í´ë³´ëŠ” ë°ì—ëŠ” ìœ ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
- Extrinsic
    - ì „ì²´ Real taskì— ëŒ€í•´ evaluate
    - ê°ê°ì˜ subtaskì¤‘ ì–´ëŠ ë¶€ë¶„ì—ì„œ ë¬¸ì œê°€ ìˆëŠ”ì§€, ì•„ë‹ˆë©´ subtask systemì˜ interactionì— ì˜í•œ ê²ƒì¸ì§€ ë“±ë“±  ì˜ˆìƒí•  ìˆ˜ ì—†ìŒ
    - ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³ , ë¹„ìš©ì´ ë§ì´ ë“¦
- Example of intrinsic word vector evaluation
    - Word vector analogy
        - syntactic / semantic analogyë¥¼ í‰ê°€
        - analogy taskë¥¼ ì˜ ìˆ˜í–‰í•˜ë©´ robustí•œ embeddingì´ë¼ê³  ì£¼ì¥
        - dimensionality, window size ë“±ì˜ hyperparameterë¥¼ ë³€ê²½ì‹œí‚¤ë©´ì„œ analogy taskì˜ ì •í™•ë„ë¥¼ ë¹„êµí•¨
        - training time, corpus source ë“±ì— ë”°ë¼ì„œë„ human knowledgeì™€ì˜ correlationì´ ë‹¬ë¼ì§
    - Human Judgement
        - Psychological
        - Human Scoring -> í‰ê· ì„ êµ¬í•´ì„œ ì‚¬ìš©
- Example of extrinsic word vector evaluation
    - ëª¨ë“  subsequent taskë¥¼ ìˆ˜í–‰í•œ ê²°ê³¼ì— ëŒ€í•œ evaluation
    - named entity recognition(ê°œì²´ëª… ì¸ì‹, NER) : ì´ë¦„ì„ ê°€ì§„ ê°œì²´ë¥¼ ì¸ì‹
    - word2vec embedding ì´í›„ì— classifyí•˜ëŠ” ê³¼ì •ì´ sequentailí•˜ê²Œ ì´ë£¨ì–´ì§
      
---

### Word Senses
- Ambiguity of word sense
- ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ëŠ” ì—¬ëŸ¬ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆìŒ
- common words
- one vectorë§Œìœ¼ë¡œ ëª¨ë“  meaningì„ captureí•  ìˆ˜ ìˆì„ê¹Œ?
- *Huang et al. 2012*, **Improving word representations via global context and multiple word prototypes**
    - word window ë‚´ì˜ ë‹¨ì–´ë“¤ì„ í´ëŸ¬ìŠ¤í„°ë§
    - ê°™ì€ ë‹¨ì–´ë„ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ì— ì†í•  ìˆ˜ ìˆìŒ
- *Arora et al. TACL 2018*, **Linear Algebraic Structure of Word Senses, with Applications to Polysemy**
    - Word2Vec ê°™ì€ word embeddingì„ í†µí•´ ì–»ì–´ì§„ word vectorì˜ superpositionìœ¼ë¡œ ìµœì¢… word vectorë¥¼ ë„ì¶œí•¨
    - ì„ í˜• ê²°í•©ì˜ ê°€ì¤‘ì¹˜ëŠ” í•´ë‹¹ ì˜ë¯¸ë¡œ ì“°ì—¬ì§„ frequencyì˜ ìƒëŒ€ì  abundanceë¡œ ê²°ì •
    - ì§ê´€ì ìœ¼ë¡œëŠ”, superpositionì„ í•˜ë©´ ì •ë³´ê°€ ì†Œì‹¤ë˜ëŠ” ê²ƒ ì•„ë‹Œê°€?
        - high-dimension word vectorëŠ” sparseí•˜ê²Œ codingë˜ì–´ ìˆìŒ
        - ê·¸ë˜ì„œ í†µê³„ì ì¸ ë°©ë²•ë¡ ì„ í™œìš©í•´ re-seperateí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ê³  ì£¼ì¥
        - [0.5, 0.0, 0.5, -1.0, -1.0, 0, 0] >> AVG([1.0, 0, 1.0, 0, 0, 0], [0, 0, 0, -2.0, -2.0, 0, 0]
