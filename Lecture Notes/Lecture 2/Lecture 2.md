# Lecture 2

### Neural Classifiers

---

### Overview : ì—¬ëŸ¬ ê°€ì§€ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸

| Word2Vec recap |
| --- |
| Word2Vec algorithm family |
| LinAlg based models : counting |
| GloVe model |
| Intrinsic/Extrinsic Evaluation of word vectors |
| Word senses |

---

### Word2Vec recap

- Word VectorëŠ” **ëœë¤í•˜ê²Œ ì´ˆê¸°í™”**ëœ ìƒíƒœ
- ì „ì²´ **corpusë¥¼ ìˆœíšŒ**í•¨, sliding window
- Modelì€ **center word vector â‡’ outer context word vectorë¥¼ ì¶”ì •**í•˜ëŠ” ì¼ì„ ìˆ˜í–‰
- ê·¸ ê³¼ì •ì—ì„œ optimizationë˜ëŠ” parameter = word vector embeddingì— í•´ë‹¹
- í•™ìŠµ ê³¼ì • = word vectorë¥¼ updateí•´ì„œ surrounding wordë¥¼ ë” ì˜ ì¶”ì •í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê³¼ì •
- P(o|c) = softmax(dot(uo, vc))ì´ë¯€ë¡œ **matrix notation**ìœ¼ë¡œë„ í‘œí˜„ ê°€ëŠ¥

<aside>
ğŸ’¡ ***Why?***

Word Vector Embedding ê²°ê³¼ë¥¼ 2D, 3D Projectioní•´ì„œ clusteringí–ˆì„ ë•Œ ìœ ì‚¬í•œ ë‹¨ì–´ë¼ë¦¬ ë¹„ìŠ·í•œ ìœ„ì¹˜ì— ì¡´ì¬í•˜ëŠ” ì´ìœ 

â‡’  ë‹¨ì–´ê°€ ì˜ë¯¸ì ìœ¼ë¡œ í˜¸ì‘í•˜ì§€ ì•Šìœ¼ë©´ ê°™ì€ window ë‚´ ë“±ì¥ì´ ì ì–´ P(o|c)ë¥¼ ì‘ê²Œ ì¶”ì •í•´ì•¼ í•˜ë¯€ë¡œ

â‡’  ë‘ word vectorì˜ dot productê°€ ê°ì†Œí•¨(projectioní•´ë„ ì„œë¡œ ë‹¤ë¥¸ ë°©í–¥ì„ ê°€ë¦¬í‚¤ê²Œ ë¨)

</aside>

- ë‹¨ì–´ì˜ ìˆœì„œì™€ ìœ„ì¹˜ëŠ” ê³ ë ¤í•˜ì§€ ì•Šê³ , ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ì— ì˜ì¡´ â†’ **Bag of Words modelê³¼ë„ ìœ ì‚¬í•¨**
- Optimization : Gradient Descent (single updateí•  ë•Œë„ ì—°ì‚°ì´ ë§ì´ í•„ìš”í•¨)
    - Stochastic Gradient Descentì„ í†µí•´ì„œ sample window í˜¹ì€ small batchì— ëŒ€í•´ì„œ updateì‹œí–‰
    - Very Sparse Gradient Vector

---

### Word2Vec algorithm family

- **Word Vector Embedding ê°œìˆ˜ì— ë”°ë¥¸ ë¶„ë¥˜**
    - ì§€ê¸ˆê¹Œì§€ ì„¤ëª…í•œ Word2Vec ëª¨ë¸ì€ **í•œ ë‹¨ì–´ë‹¹ ë‘ ê°œì˜ word vector**(center, outside(context))ê°€ ì„ë² ë”©í•¨ â‡’ ìµœì¢… word vector ì‚°ì¶œ ì‹œì—ëŠ” ë‘ word vectorì˜ í‰ê· ìœ¼ë¡œ êµ¬í•˜ê²Œ ë¨
        - ì§ê´€ì ì´ê³  ë‹¨ìˆœí•œ ë°©ë²•
    - **í•œ ë‹¨ì–´ë‹¹ í•œ ê°œì˜ word vector embedding**ì„ ìˆ˜í–‰í•˜ëŠ” variated model
        - slightly better performance
        - much complicated calculus of gradient
- **Loss functionì„ êµ¬ì„±í•˜ëŠ” ë°©ì‹ì— ë”°ë¥¸ ë¶„ë¥˜**
    - Skip-Grams(SG)
        - center wordê°€ ì£¼ì–´ì§€ë©´ â‡’ outside wordë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìˆœì„œ : P(o|c)
    - Continuous Bag of Words(CBOS)
        - context wordsë¥¼ ë°”íƒ•ìœ¼ë¡œ â‡’ center wordë¥¼ ì—ì¸¡í•˜ëŠ” ìˆœì„œ : P(c|o)
- **ê·¸ ì™¸ Additional Variations**
    - Negative Sampling
        - Naive Softmax ê°œì„  â‡’ Loss functionì— logistic term ì¶”ê°€
        - **ì•„ì´ë””ì–´ : ì¢‹ì€ ëª¨ë¸ì´ë¼ë©´ true pair(c,o)ì™€ random noise(c,o)ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤!**
            
            ![PNG á„‹á…µá„†á…µá„Œá…µ.png](Lecture%202%20b4c53a8069914dcd887f9548fbe23868/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5.png)
            

---

---

### LinAlg based models : counting

- ***COALS; Rohde, Gonnerman, & Plaut, 2005***
    - Co-occurence matrixë¥¼ êµ¬ì„± â‡’ SVD ê°™ì€ LinAlg based calculation í™œìš©
    - **Co-occurence matrix building ë°©ë²•**
        - **Sliding windows**
            - Word2Vecê³¼ ìœ ì‚¬, ê° ë‹¨ì–´ë§ˆë‹¤ windowë¥¼ ì„¤ì •í•´ slide
            - **syntactic, semantic informationì„ captureí•  ìˆ˜ ìˆìŒ**
                - syntactic ì˜ˆì‹œ : fly â‡’ flying : eat â‡’ ?
                - semantic ì˜ˆì‹œ : man â‡’ king : woman â‡’ ?
        - **Full document**
            - ì „ì²´ docì— ëŒ€í•œ co-occurence matrixëŠ” ì „ì²´ textì˜ generalí•œ topicì„ ì•Œë ¤ì¤Œ
            - **Latent Semantic Analysis(LSA)ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ**
        - Co-occurence matrix ì „ì²´ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ : But very high dimensional & Sparse
            - less robust model & sparsity issue
            - **SVD í†µí•œ ì°¨ì› ì¶•ì†Œ**
                
                ![PNG á„‹á…µá„†á…µá„Œá…µ.png](Lecture%202%20b4c53a8069914dcd887f9548fbe23868/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5%201.png)
                
    - Co-occurence matrix Xë¥¼ êµ¬ì„±í•  ë•Œ ì‚¬ìš©í•œ Hacks
        - Raw countì— ëŒ€í•´ì„œ SVDë¥¼ ìˆ˜í–‰í•˜ë©´ ì‚¬ì‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒ
        - Scaling the counts
            - ë¹ˆë„ê°€ ë„ˆë¬´ ë†’ì€ function wordsì œì™¸
            - ë¹ˆë„ = max(X,100)ìœ¼ë¡œ thresholdí•˜ê¸°
            - log(ë¹ˆë„)ë¡œ ê³„ì‚°
        - ë‹¨ìˆœíˆ count ì“°ê¸°ë³´ë‹¤ Pearson correlation ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ë¥´ê²Œ(negative vals â‡’ 0)
        - ì¤‘ì‹¬ì—ì„œ ê°€ê¹ê²Œ ë‚˜íƒ€ë‚ ìˆ˜ë¡ ë” ë†’ì€ ê°€ì¤‘ì¹˜ : Ramped Windows(ğŸ”º)

---

### GloVe Model

- **ì§€ê¸ˆê°€ì§€ ì†Œê°œí•œ ëª¨ë¸ë“¤ ë¶„ë¥˜í•´ì„œ ì •ë¦¬í•œ ê²°ê³¼**
    
    
    | Count Based | Direct Prediction |
    | --- | --- |
    | LSA(Latent Semantic Analaysis), HAL | Skip-Gram & CBOW |
    | COALS, Hellinger-PCA | NNLM, HLBL, RNN |
    | ì¥ì  1 : Fast Training | ì¥ì  1 : Improved performance |
    | ì¥ì  2 : Efficient Usage of statistics | ì¥ì  2 : Capture complex patterns |
    | ë‹¨ì  1 : ë‹¨ìˆœí•œ Word similarity ì •ë„ë§Œ capture ê°€ëŠ¥ | ë‹¨ì  1 : Corpus Sizeì— ë”°ë¼ í•™ìŠµ ì†ë„ ëŠë ¤ì§ |
    | ë‹¨ì  2 : ë¹ˆë„ê°€ ë†’ê²Œ ì¸¡ì •ëœ ë‹¨ì–´ì— importance ë¶ˆê· ë“±í™” | ë‹¨ì  2 : Inefficient Usage of Statistics |
- ***Encoding meaning components in vector differences [Pennington et al, EMNLP 2014]***
    - Co-occurence probabilityì˜ ë¹„ìœ¨ì´ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸ì½”ë”©í•  ê²ƒì´ë¼ê³  ì¶”ë¡ í•¨
        
        ![PNG á„‹á…µá„†á…µá„Œá…µ.png](Lecture%202%20b4c53a8069914dcd887f9548fbe23868/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5%202.png)
        
    - Counting í†µí•´ì„œ embedding vector ê²Œì‚°í•˜ê±°ë‚˜ ìœ ì¶” ê°€ëŠ¥
        
        ![PNG á„‹á…µá„†á…µá„Œá…µ.png](Lecture%202%20b4c53a8069914dcd887f9548fbe23868/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5%203.png)
        
- ***GloVe [Pennington et al, EMNLP 2014]***
    
    ![PNG á„‹á…µá„†á…µá„Œá…µ.png](Lecture%202%20b4c53a8069914dcd887f9548fbe23868/PNG_%E1%84%8B%E1%85%B5%E1%84%86%E1%85%B5%E1%84%8C%E1%85%B5%204.png)
    
    - Fast Training / Scalabilityê°€ ì¥ì 
    - ì‘ì€ ê·œëª¨ì˜ Corpusì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

---

### Intrinsic/Extrinsic evaluation of word vectors

---

### Word Senses